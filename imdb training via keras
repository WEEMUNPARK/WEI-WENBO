from __future__ import print_function
from sklearn.model_selection import train_test_split
import keras.regularizers
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM
from keras.datasets import imdb
import numpy as np
from keras.optimizers import SGD

#set up hyperparameters
max_features = 50000
maxlen=500
batch_size = 128

print('Loading data...') #just in case accident happens, can find out the problem more easily(in which part)

(train_data,train_labels),(test_data,test_labels) = imdb.load_data(num_words=50000)
a=np.append(train_data,test_data)
b=np.append(train_labels,test_labels)
x_train=a[:35000]
y_train=b[:35000]
x_test=a[35000:]
y_test=b[35000:]

x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

print('Build model...')

#set up keras LSTM nn
model = Sequential()
model.add(Embedding(max_features, 32))
model.add(LSTM(60, dropout=0.1, kernel_regularizer=keras.regularizers.l2(0.001)))
model.add(Dense(1, activation='sigmoid'))
opt = keras.optimizers.Adam(lr=0.005)

model.compile(loss='binary_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])
print('Train...')

#start training (epochs can be changed)
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=10,
          validation_data=(x_test, y_test))
score, acc = model.evaluate(x_test, y_test,
                            batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)
